# Reinforcement Learning in a Supply Chain Inventory Management Problem.

In this study, I used OR-Gym from OpenAI Gym to simulate the supply chain inventory management problem and introduced an autonomous intelligent agent to solve the problem using RL. I was motivated to work on this topic after learning about inventory management problems in my university’s Supply Chain and Operations Planning module. It can contribute to a broader understanding of using RL algorithms in supply chain management, how they can be utilized to enhance decision-making processes, and improve the design of inventory management systems and their performance.

Discount rates are a critical component of reinforcement learning algorithms that determine how much importance the agent places between immediate and future rewards. In the context of inventory management systems, RL algorithms can be used to optimize inventory policies and reduce total costs. The discount rate significantly affects the agent’s decision-making process, balancing immediate inventory holding costs with potential future cost of stockouts or backorders. Investigating the impact of different discount rates on RL-based inventory management can help gain insight into the balance between short and long-term rewards. This can help optimize the inventory management process and improve overall supply chain performance.

The algorithm implemented in this study builds on the existing, classic Q-learning algorithm using built-in Python libraries like numpy and defaultdict to define and update the Q-table, which is a central component of many RL algorithms. The Q-table stores the Q-values for each state-action pair, where each state corresponds to a combination of the current inventory level, order level, time until next shipment, and time step. The Q-value represents the expected future reward of taking a particular action in a particular state. My custom implementation algorithm uses a similar update rule to that of the standard Q-Learning, but with a few modifications. A difference from the standard algorithm is that in this study, the Q-values are stored in a defaultdict instead of a numpy array. Besides that, it applies an ε-greedy exploration strategy to balance exploration and exploitation. This strategy allows the agent a chance to choose a random action (exploration) with probability equal to the exploration rate, otherwise it will choose the action with the highest Q-value (exploitation).
